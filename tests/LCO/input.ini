[Overview]
Model = IDNN 
Data_Generation = True 
Data_Generation_Source = CASM
Restart = False 
Input_data = False
Input_Dim = 8
Output_Dim = 1 
Derivative_Dim = 7 
Iterations = 3 
OutputFolder = /Users/jamieholber/Software/active-learning/Output/
Seed = 1
Temperatures = 260,300,340


[CASM Data Generation] 
casm_project_dir = /Users/jamieholber/Software/active-learning/tests/LCO/CASMcode
CASM_version = LCO 
Job_Manager = PC
Account = mia341
Walltime = 2:00:00
Mem = 10G
Initial_mu = Ideal
Phi =10.,0.1,0.1,0.1,0.1,0.1,0.1
N_jobs = 1
surrogate = True

[CASM Surrogate]
Hidden_Layers = 174,174,174
Input_Shape = 1,12
dim = 7
version = 'LCO'
Activation = 'tanh'
Transforms_directory = '/Users/jamieholber/Software/active-learning/tests/LCO'


[Custom Data Generation]
Generation_File = ''

[Exploit Parameters]
hessian = True
hessian_repeat=3,2
hessian_repeat_points=20,10
high_error = True
high_error_repeat=3,2
high_error_repeat_points=20,10

[Sampling Domain]
Domain = 0,1
Global_Points = 10
Wells = 
Sample_Wells = None
Sample_Vertices = False
test_set = billiardwalk
x0 = 0.5,0,0,0,0,0,0
Q_matrix = /Users/jamieholber/Software/active-learning/tests/LCO/Q.txt


[Training]
WeightRecent=No
LR_decay = 0.95
Min_lr = 1.e-6
EarlyStopping = No
Patience = 150

[Hyperparameters]


[Neural Network]
Epochs = 2
Batch_size = 500
Activation = tanh
Hidden_Units = 174,174,174
Optimizer = legacy.RMSprop
N_sets = 3
LearningRate = 0.001,0.5
Layers = 2,3
Neurons = 20,200
Dropout = 0.06
Learning =  0.002

[Inference]
#Parameters TBD


[Restart]
Restart_path = '/' 