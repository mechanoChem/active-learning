[Main]
#This section defines the main parameters needed for the workflow.

#This is the type of model that will be trained
Model = IDNN 
#False if user only wants to recommended data points (ie experiments)
Data_Generation = True
#if Data_Generation = True, this field indicates the data generation source ie CASM, CASM Surrogate, Custom
Data_Generation_Source = CASM_Surrogate
#If you want to restart a pre-existing run, it picks up where the last run ended
restart =False
#if there is prexisting data you want to use to train your model, this should be True - ie 
Input_data = False 
#These are the parameters that serve as the input to our model, corresponding sections must be created for each parameter
input_alias = eta,temp
#These are the parameters that serve as the output to our model, corresponding sections must be created for each parameter
Output_alias = mu
#This is the number of iterations for the active learning workflow. This does not include the 0th round where only the explore 
#sampling occurs
iterations = 5
OutputFolder = /Users/jamieholber/Desktop/Software/active-learning/tests/LCO_zigzag/surrogate_model/
#Used in sobol sequence to define randomness 
seed = 1
temp = 300
#matlab, python, both, neither
graph = python
Reweight = True
reweight_alpha = 1000
Prediction_points = 100000

#This section is needed if data_generation_source = CASM, 
#otherwise if data_generation_source = 'something', then a section called 'something' is needed, with different parameters
[CASM] 
#location of folder to run CASM monte carlo
casm_project_dir = /home/jholber/CASM_sampling/row
#version of CASM biasing
CASM_version = row
#The first guess for mu for casm biasing. kappa = eta + .5mu/phi. If initial_mu = none, mu = 0, can also choose initial_mu = ideal.
#This slightly changes what points are calculated using the explore data recommender for round 0, but does not change the accuracy of resuts
initial_mu = none
#Phi needed for umbrella sampling
phi =10.,0.1,0.1,0.1,0.1,0.1,0.1
#Number of jobs to break all samping into. 
n_jobs = 1

#This section is needed if data_generation_source = CASM_Surrogate (use previously trained model) 
#a subfolder called 'surrogate_weights' of the trained model must be in the same folder as this ini file
[CASM_Surrogate]
casm_project_dir =/Users/jamieholber/Desktop/Software/active-learning/tests/LCO_zigzag
CASM_version = 0.3.X
initial_mu = none
phi =10.,0.1,0.1,0.1,0.1,0.1,0.1
n_jobs = 1
#Hidden layers in trained model
hidden_layers = 174,174
#input_shape for trained model
input_shape = 1,12
#dim of trained model
dim = 7
#version of trained model
version =  0.3.X
#activation function of trained model
activation = tanh
#transforms directory for trained model
transforms_directory = /Users/jamieholber/Desktop/Software/active-learning/tests/LCO
relevent_indices = 0,1,2,3,4,5,6


#This section must be included
[Explore_Parameters]
#Number of global points for space filling sampling
global_points = 570
#True/False field, samsple known wells
sample_known_wells = False
#txt file containing a set of points at the wells, should include all input variables in the same order as the input_alias
wells = wells_2.npy
#number of points to sample at each well
wells_points = 25
#True/False field, sample known vertices
sample_known_vertices = False
#txt file containing a set of points at the vertices, should include all input variables in the same order as the input_alias
vertices = vertices_2.npy
#number of points to sample at each vertex
vertices_points = 25

#This section must be included
[Exploit_Parameters]

#True/False field- true if you want to sample additional points in regions of non-convexities
hessian = False
#if hessian= True, then points are sorted by the strength/proximity of their non-convexity
#comma deliminated list, dividing the points into groups. The hessian_repeat_points indicates
#the number of points in each group. hessian_repeat indicates the number of times the point
#in each group should be perturbed to resample.
hessian_repeat=3,2
hessian_repeat_points=100,50

#True/False field - true if you want to sample points based on the error between the model and the provided data points
high_error = True
#if high_error = true, points are sorted by their error (high to low). high_error_repeat_points indicates the number of
#points in each group. high_error_repeat indicates the number of times the point
#in each group should be perturbed to resample.
high_error_repeat=3,2
high_error_repeat_points=200,200

#True/False field - true if you want to find wells. 
find_wells = False
#if find_wells = true, points are sorted by their gradient norm (low to high). wells_repeat_points indicates the 
#number of points in each group. wells_repeat indicates the number of times the point in each group should be perturbed to resample.
wells_repeat=3,2
wells_repeat_points=40,20

#True/False field - true if you want to sample along the lowest free energy curve
lowest_free_energy = True
lowest_repeat = 3
lowest_file =eta_curve_zigzag.txt

#True/False field- true if you want to sample additional points in regions of non-convexities
sensitivity = True
#if hessian= True, then points are sorted by the strength/proximity of their non-convexity
#comma deliminated list, dividing the points into groups. The hessian_repeat_points indicates
#the number of points in each group. hessian_repeat indicates the number of times the point
#in each group should be perturbed to resample.
sensitivity_repeat=3,2
sensitivity_repeat_points=100,50
#True/False field- true if you want to sample additional points in regions of non-convexities
QBC = False
#if non_convexities= True, then points are sorted by the strength/proximity of their non-convexity
#comma deliminated list, dividing the points into groups. The non_convexities_repeat_points indicates
#the number of points in each group. non_convexities_repeat indicates the number of times the point
#in each group should be perturbed to resample.
QBC_points = 1000


#first input parameter 
[eta]
#there are 3 input parameter domain types : continous dependent, continous independent, and discrete 
#continous_dependent means that the domain of one parameter is dependent on other parameters's values. 
#All points with dependecies are grouped into a domain. ie etasampling, which must be defined. 
#Multiple parameters can be grouped together, and all refer to the same domain
domain_type = continuous_dependent
#domain name, refering to section below
domain = etasampling
#This parameter is only relvent to the IDNN. The IDNN works by taking the derivative of the neural network wrt
#some subset of parameters. This indicates whether or not this is one of these parameters
derivative_dim = True
#Number of dimensions for this parameter
dimensions = 7
#Whether or not to adjust the parameter value to train the model. param = (param+x0)*x1 Ie eta' =  (eta + 0)*1
adjust=0,1

[temp]
#domain type of discrete, means that this parameter can only take discrete values
domain_type = discrete
#the domain is the discrete values it could take
domain = 260
derivative_dim = False
dimensions = 1
adjust = -260,.025


#example of an output variable (supplied by data sampling)
[mu]
#for use in IDNN, indicates if this is 0th derivative, 1st derivative, or 2nd derivative variable. 
derivative=1
#number of dimensions
dimensions = 7
adjust=0,100



[etasampling]
#example of domain corresponding to some subset of parameters.
#space_filling_method can be billiardwalk/sobol sequence
space_filling_method = billiardwalk 
#starting point for space filling method
x0 = 0.5,0,0,0,0,0,0
#additional file/folder needed for the domain. For billiardwalk it is the Q matrix
filepath = Q.txt


#job manager to perform data sampling
[Sampling_Job_Manager]
Job_Manager = PC
#if job manager != PC, need account, walltime and memory
Account = mia346
Walltime = 12:00:00
Mem = 10G

#job manager to do hyperparameter search
[Hyperparameter_Job_Manager]
Job_Manager = PC
Account = mia346
Walltime = 8:00:00
Mem = 10G


[IDNN]
#model definition parameters
layers=2
neurons=290
activation=tanh,tanh,tanh
dropout = 0.06
Transforms_directory =/Users/jamieholber/Desktop/Software/active-learning/tests/LCO_zigzag/

#compile parameters
#loss and loss weights are defined for each neural network derivative (0,1,2)
loss = mse,mse,None
loss_weights= 0.01,0.1,0
optimizer = RMSprop
learning =  0.0005
lr_decay = 0.95

#model fit parameters 
factor=0.5
patience = 150
min_lr = 1.e-6
EarlyStopping = No
Epochs = 1000
Batch_size = 50
WeightRecent=No
validation_split = 0.25

#True/False field, true if you want to do hyperparameter search
IDNN_Hyperparameter = True

[IDNN_Hyperparameter]
#subset of fields to update in the hyperameter search, and the range to search over- based on parameter will be either list, or range
n_sets = 10

#model definition parameters
layers = 3,4
neurons =20,200

activation = tanh,tanh
; dropout = 0.05,0.08

; #compile parameters
; optimizer = legacy.RMSprop,legacy.Adam
learning = 0.0001,0.0005
; lr_decay = 0.90,0.95


; #model fit parameters 
; Factor=0.5,0.6
; Patience = 100,150
; Min_lr = 1.e-6,5.e-6
; Epochs = 2,4
; Batch_size = 100,500


[Inference]
#Parameters TBD


[Restart]
#If restarting, what rnd to restart on
rnd = 0
step = Exploitative
